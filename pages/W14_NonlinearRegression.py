# W14_NonlinearRegression.py
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

st.set_page_config(layout="wide")
st.title("ç¬¬ 14 é€±ï½œéç·šæ€§è¿´æ­¸èˆ‡å¤šé …å¼å›æ­¸")
st.caption("ğŸ“˜ æ•™ç§‘æ›¸ç¬¬ 10 ç« ï½œéç·šæ€§æ“¬åˆèˆ‡å·¥ç¨‹é æ¸¬")

np.random.seed(42)
X = np.linspace(50, 200, 25).reshape(-1, 1)
noise = np.random.normal(0, 2, 25)
Y = (0.01*X**2 - 0.5*X + 10).flatten() + noise

degree = st.slider("å¤šé …å¼éšæ•¸", 1, 5, 2)

poly = PolynomialFeatures(degree=degree)
X_poly = poly.fit_transform(X)

model = LinearRegression()
model.fit(X_poly, Y)

Y_pred = model.predict(X_poly)
Y_pred = Y_pred.flatten()  # ç¢ºä¿é æ¸¬å€¼æ˜¯ä¸€ç¶­é™£åˆ—

X_flat = X.flatten()  # ç¢ºä¿ X æ˜¯ä¸€ç¶­é™£åˆ—

residuals = Y - Y_pred

# é¡¯ç¤ºä¿‚æ•¸
# PolynomialFeatures ç”¢ç”Ÿ [1, x, x^2, ..., x^degree]
# LinearRegression çš„ intercept_ å°æ‡‰æˆªè·ï¼Œcoef_ å°æ‡‰æ‰€æœ‰ç‰¹å¾µ(åŒ…æ‹¬å¸¸æ•¸é …1)
# æ‰€ä»¥ coef_[0] å°æ‡‰å¸¸æ•¸é …1çš„ä¿‚æ•¸(é€šå¸¸æ¥è¿‘0)ï¼Œcoef_[1:] å°æ‡‰ x, x^2, ..., x^degree
coefficients = np.concatenate([
    np.atleast_1d(model.intercept_), 
    np.atleast_1d(model.coef_).flatten()[1:]
])
st.write(f"è¿´æ­¸ä¿‚æ•¸ b0~b{degree}:", np.round(coefficients, 3))
st.write(f"RÂ² = {r2_score(Y, Y_pred):.3f}")

# æ•£ä½ˆåœ–èˆ‡æ“¬åˆæ›²ç·š
fig, ax = plt.subplots()
ax.scatter(X_flat, Y, color='blue', label='å¯¦æ¸¬å€¼')
ax.plot(X_flat, Y_pred, color='red', label=f'{degree}æ¬¡å¤šé …å¼æ“¬åˆ')
ax.set_xlabel("X")
ax.set_ylabel("Y")
ax.legend()
st.pyplot(fig)

# æ®˜å·®åœ–
fig2, ax2 = plt.subplots()
ax2.scatter(range(len(residuals)), residuals)
ax2.axhline(0, color='red', linestyle='--')
ax2.set_xlabel("æ¨£æœ¬ç·¨è™Ÿ")
ax2.set_ylabel("æ®˜å·®")
ax2.set_title("æ®˜å·®åœ–")
st.pyplot(fig2)
